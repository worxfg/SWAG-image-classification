{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac47ee35-c8d8-4e5f-bc4e-31a777244906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_7588\\102984405.py:373: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.network.load_state_dict(torch.load(PRETRAINED_WEIGHTS_FILE))\n",
      "C:\\Users\\Alex\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained MAP weights from C:\\Users\\Alex\\Desktop\\Alex\\Machine learning\\pour github\\task2_handout_e14a688d - Copie - Copie\\map_weights.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent for SWA: 100%|██████████| 30/30 [11:49<00:00, 23.64s/it, lr=0.045, avg. epoch loss=0.347, avg. epoch accuracy=0.876]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on validation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing Bayesian model averaging: 100%|██████████| 30/30 [05:03<00:00, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (raw): 0.6357\n",
      "Accuracy (non-ambiguous only, your predictions): 0.6250\n",
      "Accuracy (non-ambiguous only, predicting most-likely class): 0.8750\n",
      "Best cost 0.5642856955528259 at threshold 0.6681299805641174\n",
      "Note that this threshold does not necessarily generalize to the test set!\n",
      "Validation ECE: 0.14296421451228006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "import collections\n",
    "import enum\n",
    "import math\n",
    "import pathlib\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from util import draw_reliability_diagram, cost_function, setup_seeds, calc_calibration_curve\n",
    "\n",
    "EXTENDED_EVALUATION = False\n",
    "\"\"\"\n",
    "Set `EXTENDED_EVALUATION` to `True` in order to generate additional plots on validation data.\n",
    "\"\"\"\n",
    "\n",
    "USE_PRETRAINED_MODEL = True\n",
    "\"\"\"\n",
    "If `USE_PRETRAINED_MODEL` is `True`, then MAP inference uses provided pretrained weights.\n",
    "If the constant is set to `False`,\n",
    "this solution always performs MAP inference before running SWAG.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    data_location = pathlib.Path.cwd()\n",
    "    model_location = pathlib.Path.cwd()\n",
    "    output_location = pathlib.Path.cwd()\n",
    "\n",
    "    # Load training data\n",
    "    training_images = torch.from_numpy(np.load(data_location / \"train_xs.npz\")[\"train_xs\"])\n",
    "    training_metadata = np.load(data_location / \"train_ys.npz\")\n",
    "    training_labels = torch.from_numpy(training_metadata[\"train_ys\"])\n",
    "    training_snow_labels = torch.from_numpy(training_metadata[\"train_is_snow\"])\n",
    "    training_cloud_labels = torch.from_numpy(training_metadata[\"train_is_cloud\"])\n",
    "    training_dataset = torch.utils.data.TensorDataset(training_images, training_snow_labels, training_cloud_labels, training_labels)\n",
    "\n",
    "    # Load validation data\n",
    "    validation_images = torch.from_numpy(np.load(data_location / \"val_xs.npz\")[\"val_xs\"])\n",
    "    validation_metadata = np.load(data_location / \"val_ys.npz\")\n",
    "    validation_labels = torch.from_numpy(validation_metadata[\"val_ys\"])\n",
    "    validation_snow_labels = torch.from_numpy(validation_metadata[\"val_is_snow\"])\n",
    "    validation_cloud_labels = torch.from_numpy(validation_metadata[\"val_is_cloud\"])\n",
    "    validation_dataset = torch.utils.data.TensorDataset(validation_images, validation_snow_labels, validation_cloud_labels, validation_labels)\n",
    "\n",
    "    # Fix all randomness\n",
    "    setup_seeds()\n",
    "\n",
    "    # Build and run the actual solution\n",
    "    training_loader = torch.utils.data.DataLoader(\n",
    "        training_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    swag_inference = SWAGInference(\n",
    "        train_xs=training_dataset.tensors[0],\n",
    "        model_dir=model_location,\n",
    "    )\n",
    "    swag_inference.fit(training_loader)\n",
    "    swag_inference.apply_calibration(validation_dataset)\n",
    "\n",
    "    # fork_rng ensures that the evaluation does not change the rng state.\n",
    "    with torch.random.fork_rng():\n",
    "        evaluate(swag_inference, validation_dataset, EXTENDED_EVALUATION, output_location)\n",
    "\n",
    "\n",
    "class InferenceType(enum.Enum):\n",
    "    \"\"\"\n",
    "    `MAP` simply predicts the most likely class using pretrained MAP weights.\n",
    "    `SWAG_DIAGONAL` and `SWAG_FULL` correspond to SWAG-diagonal and the full SWAG method, respectively.\n",
    "    \"\"\"\n",
    "    MAP = 0\n",
    "    SWAG_DIAGONAL = 1\n",
    "    SWAG_FULL = 2\n",
    "\n",
    "\n",
    "class SWAGInference(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_xs: torch.Tensor,\n",
    "        model_dir: pathlib.Path,\n",
    "        inference_mode: InferenceType = InferenceType.SWAG_FULL,\n",
    "\n",
    "        swag_training_epochs: int = 30,\n",
    "        swag_lr: float = 0.045,\n",
    "        swag_update_interval: int = 1,\n",
    "        max_rank_deviation_matrix: int = 15,\n",
    "        num_bma_samples: int = 30,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param train_xs: Training images (for storage only)\n",
    "        :param model_dir: Path to directory containing pretrained MAP weights\n",
    "        :param inference_mode: Control which inference mode (MAP, SWAG-diagonal, full SWAG) to use\n",
    "        :param swag_training_epochs: Total number of gradient descent epochs for SWAG\n",
    "        :param swag_lr: Learning rate for SWAG gradient descent\n",
    "        :param swag_update_interval: Frequency (in epochs) for updating SWAG statistics during gradient descent\n",
    "        :param max_rank_deviation_matrix: Rank of deviation matrix for full SWAG\n",
    "        :param num_bma_samples: Number of networks to sample for Bayesian model averaging during prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.inference_mode = inference_mode\n",
    "        self.swag_training_epochs = swag_training_epochs\n",
    "        self.swag_lr = swag_lr\n",
    "        self.swag_update_interval = swag_update_interval\n",
    "        self.max_rank_deviation_matrix = max_rank_deviation_matrix\n",
    "        self.num_bma_samples = num_bma_samples\n",
    "\n",
    "        # Network used to perform SWAG.\n",
    "        # All operations in this class modify this network in-place\n",
    "        self.network = CNN(in_channels=3, out_classes=6)\n",
    "\n",
    "        # Store training dataset to recalculate batch normalization statistics during SWAG inference\n",
    "        self.training_dataset = torch.utils.data.TensorDataset(train_xs)\n",
    "\n",
    "        #create attributes for SWAG-diagonal\n",
    "        self.mean_weights = self._create_weight_copy()  \n",
    "        self.sq_mean_weights = self._create_weight_copy()  \n",
    "        self.num_snapshots = 0  \n",
    "\n",
    "        #create attributes for SWAG-full\n",
    "        self.deviation_vectors = collections.deque(maxlen=self.max_rank_deviation_matrix)\n",
    "\n",
    "        # Calibration, prediction, and other attributes\n",
    "        self._calibration_threshold = None\n",
    "        \n",
    "    def update_swag_statistics(self) -> None:\n",
    "        \"\"\"\n",
    "        Update SWAG statistics with the current weights of self.network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a copy of the current network weights\n",
    "        copied_params = {name: param.detach() for name, param in self.network.named_parameters()}\n",
    "\n",
    "        # SWAG-diagonal\n",
    "        for name, param in copied_params.items():\n",
    "            \n",
    "            old_mean = self.mean_weights[name]\n",
    "            self.mean_weights[name] = (old_mean*self.num_snapshots + param)/ (self.num_snapshots+1)  \n",
    "\n",
    "            old_msqd =  self.sq_mean_weights[name]\n",
    "            self.sq_mean_weights[name] = (old_msqd*self.num_snapshots + param**2)/ (self.num_snapshots+1)            \n",
    "            \n",
    "        self.num_snapshots += 1\n",
    "\n",
    "        # Full SWAG\n",
    "        if self.inference_mode == InferenceType.SWAG_FULL:\n",
    "\n",
    "            deviation = {name: param - self.mean_weights[name] for name, param in copied_params.items()}\n",
    "            self.deviation_vectors.append(deviation)\n",
    "            \n",
    "            # Limit the number of stored deviations to max_rank_deviation_matrix\n",
    "            if len(self.deviation_vectors) > self.max_rank_deviation_matrix:\n",
    "                self.deviation_vectors.popleft()\n",
    "\n",
    "    def fit_swag_model(self, loader: torch.utils.data.DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Fit SWAG on top of the pretrained network self.network.\n",
    "        \"\"\"\n",
    "\n",
    "        # We use SGD with momentum and weight decay to perform SWA.\n",
    "        # See the paper on how weight decay corresponds to a type of prior.\n",
    "        \n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.network.parameters(),\n",
    "            lr=self.swag_lr,\n",
    "            momentum=0.9,\n",
    "            nesterov=False,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        \n",
    "        # By default, this scheduler just keeps the initial learning rate given to `optimizer`.\n",
    "        lr_scheduler = SWAGScheduler(\n",
    "            optimizer,\n",
    "            epochs=self.swag_training_epochs,\n",
    "            steps_per_epoch=len(loader),\n",
    "        )\n",
    "\n",
    "        # Perform initialization for SWAG fitting\n",
    "        self.num_snapshots = 0\n",
    "        for name in self.mean_weights:\n",
    "            self.mean_weights[name].zero_()\n",
    "            self.sq_mean_weights[name].zero_()\n",
    "\n",
    "        self.network.train()\n",
    "        with tqdm.trange(self.swag_training_epochs, desc=\"Running gradient descent for SWA\") as pbar:\n",
    "            progress_dict = {}\n",
    "            for epoch in pbar:\n",
    "                avg_loss = 0.0\n",
    "                avg_accuracy = 0.0\n",
    "                num_samples = 0\n",
    "                for batch_images, batch_snow_labels, batch_cloud_labels, batch_labels in loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    predictions = self.network(batch_images)\n",
    "                    batch_loss = loss_fn(input=predictions, target=batch_labels)\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    progress_dict[\"lr\"] = lr_scheduler.get_last_lr()[0]\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "                    # Calculate cumulative average training loss and accuracy\n",
    "                    avg_loss = (batch_images.size(0) * batch_loss.item() + num_samples * avg_loss) / (\n",
    "                        num_samples + batch_images.size(0)\n",
    "                    )\n",
    "                    avg_accuracy = (\n",
    "                        torch.sum(predictions.argmax(dim=-1) == batch_labels).item()\n",
    "                        + num_samples * avg_accuracy\n",
    "                    ) / (num_samples + batch_images.size(0))\n",
    "                    num_samples += batch_images.size(0)\n",
    "                    progress_dict[\"avg. epoch loss\"] = avg_loss\n",
    "                    progress_dict[\"avg. epoch accuracy\"] = avg_accuracy\n",
    "                    pbar.set_postfix(progress_dict)\n",
    "\n",
    "                # Implement periodic SWAG updates\n",
    "                if epoch % self.swag_update_interval == 0:\n",
    "                    self.update_swag_statistics()\n",
    "\n",
    "    def apply_calibration(self, validation_data: torch.utils.data.Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Calibrate predictions using a small validation set.\n",
    "        validation_data contains well-defined and ambiguous samples,\n",
    "        where the latter should by identified by having label -1.\n",
    "        \"\"\"\n",
    "        if self.inference_mode == InferenceType.MAP:\n",
    "            # In MAP mode, simply predict argmax and do nothing else\n",
    "            self._calibration_threshold = 0.0\n",
    "            return\n",
    "\n",
    "        # prediction threshold\n",
    "        self._calibration_threshold = 2.0 / 3.0\n",
    "\n",
    "        val_images, val_snow_labels, val_cloud_labels, val_labels = validation_data.tensors\n",
    "        assert val_images.size() == (140, 3, 60, 60)  # N x C x H x W\n",
    "        assert val_labels.size() == (140,)\n",
    "        assert val_snow_labels.size() == (140,)\n",
    "        assert val_cloud_labels.size() == (140,)\n",
    "\n",
    "    def predict_probabilities_swag(self, loader: torch.utils.data.DataLoader) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform Bayesian model averaging using SWAG statistics and predict\n",
    "        probabilities for all samples in the loader.\n",
    "        Outputs should is an Nx6 tensor, where N is the number of samples in loader,\n",
    "        and all rows of the output sum to 1.\n",
    "        That is, output row i column j should is our predicted p(y=j | x_i).\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.eval()\n",
    "\n",
    "        # Perform Bayesian model averaging:\n",
    "        model_predictions = []\n",
    "        for _ in tqdm.trange(self.num_bma_samples, desc=\"Performing Bayesian model averaging\"):\n",
    "            # Sample new parameters for self.network from the SWAG approximate posterior\n",
    "\n",
    "            self.sample_parameters()\n",
    "            all_predictions = []\n",
    "\n",
    "            # Perform inference for all samples in `loader` using current model sample,\n",
    "            # and add the predictions to model_predictions\n",
    "            for (batch_images,) in loader:\n",
    "                 all_predictions.append(self.network(batch_images))\n",
    "            model_predictions.append(torch.softmax(torch.cat(all_predictions), dim=-1))\n",
    "\n",
    "        assert len(model_predictions) == self.num_bma_samples\n",
    "        assert all(\n",
    "            isinstance(sample_predictions, torch.Tensor)\n",
    "            and sample_predictions.dim() == 2  # N x C\n",
    "            and sample_predictions.size(1) == 6\n",
    "            for sample_predictions in model_predictions\n",
    "        )\n",
    "\n",
    "        # Average predictions from different model samples into bma_probabilities\n",
    "        bma_probabilities = torch.mean(torch.stack(model_predictions), dim=0)\n",
    "\n",
    "\n",
    "        assert bma_probabilities.dim() == 2 and bma_probabilities.size(1) == 6  # N x C\n",
    "        return bma_probabilities\n",
    "\n",
    "    def sample_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Sample a new network from the approximate SWAG posterior.\n",
    "        For simplicity, this method directly modifies self.network in-place.\n",
    "        Hence, after calling this method, self.network corresponds to a new posterior sample.\n",
    "        \"\"\"\n",
    "\n",
    "        for name, param in self.network.named_parameters():\n",
    "            # SWAG-diagonal part\n",
    "            z_diag = torch.randn(param.size())\n",
    "            mean_weights = self.mean_weights[name]\n",
    "            variance_weights = self.sq_mean_weights[name] - self.mean_weights[name] ** 2\n",
    "            std_weights = torch.sqrt(variance_weights.clamp(min=1e-8))\n",
    "            assert mean_weights.size() == param.size() and std_weights.size() == param.size()\n",
    "\n",
    "            sampled_weight = mean_weights + std_weights * z_diag/math.sqrt(2)\n",
    "\n",
    "            # Full SWAG part\n",
    "            if self.inference_mode == InferenceType.SWAG_FULL:\n",
    "                z_off_diag = torch.randn(len(self.deviation_vectors))\n",
    "                for i, deviation in enumerate(self.deviation_vectors):\n",
    "                    sampled_weight += z_off_diag[i] * deviation[name] / math.sqrt(2 * (self.max_rank_deviation_matrix - 1))\n",
    "\n",
    "            # Modify weight value in-place; directly changing self.network\n",
    "            param.data = sampled_weight\n",
    "\n",
    "        self._update_batchnorm_statistics()\n",
    "\n",
    "\n",
    "    def predict_labels(self, predicted_probabilities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict labels in {0, 1, 2, 3, 4, 5} or \"don't know\" as -1\n",
    "        based on our model's predicted probabilities.\n",
    "        The parameter predicted_probabilities is an Nx6 tensor containing predicted probabilities\n",
    "        as returned by predict_probabilities(...).\n",
    "        The output is a N-dimensional long tensor, containing values in {-1, 0, 1, 2, 3, 4, 5}.\n",
    "        \"\"\"\n",
    "\n",
    "        # label_probabilities contains the per-row maximum values in predicted_probabilities,\n",
    "        # max_likelihood_labels the corresponding column index (equivalent to class).\n",
    "        label_probabilities, max_likelihood_labels = torch.max(predicted_probabilities, dim=-1)\n",
    "        num_samples, num_classes = predicted_probabilities.size()\n",
    "        assert label_probabilities.size() == (num_samples,) and max_likelihood_labels.size() == (num_samples,)\n",
    "\n",
    "        # A model without uncertainty awareness might simply predict the most likely label per sample:\n",
    "        # return max_likelihood_labels\n",
    "        \n",
    "        # Calculate the second-highest probability for each sample\n",
    "        sorted_probs, _ = torch.sort(predicted_probabilities, dim=-1, descending=True)\n",
    "        second_highest_prob = sorted_probs[:, 1]\n",
    "\n",
    "        # Define an ambiguity threshold (adjustable)\n",
    "        ambiguity_threshold = 0.1  \n",
    "        # Use both the calibration threshold and ambiguity check to determine the output\n",
    "        dont_know_mask = (label_probabilities < self._calibration_threshold) | \\\n",
    "                     ((label_probabilities - second_highest_prob) < ambiguity_threshold)\n",
    "\n",
    "        return torch.where(\n",
    "            dont_know_mask,\n",
    "            torch.ones_like(max_likelihood_labels) * -1,\n",
    "            max_likelihood_labels,\n",
    "        )\n",
    "\n",
    "    def _create_weight_copy(self) -> typing.Dict[str, torch.Tensor]:\n",
    "        \"\"\"Create an all-zero copy of the network weights as a dictionary that maps name -> weight\"\"\"\n",
    "        return {\n",
    "            name: torch.zeros_like(param, requires_grad=False)\n",
    "            for name, param in self.network.named_parameters()\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        loader: torch.utils.data.DataLoader,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Perform full SWAG fitting procedure.\n",
    "        If `PRETRAINED_WEIGHTS_FILE` is `True`, this method skips the MAP inference part,\n",
    "        and uses pretrained weights instead.\n",
    "        \"\"\"\n",
    "\n",
    "        # MAP inference to obtain initial weights\n",
    "        PRETRAINED_WEIGHTS_FILE = self.model_dir / \"map_weights.pt\"\n",
    "        if USE_PRETRAINED_MODEL:\n",
    "            self.network.load_state_dict(torch.load(PRETRAINED_WEIGHTS_FILE))\n",
    "            print(\"Loaded pretrained MAP weights from\", PRETRAINED_WEIGHTS_FILE)\n",
    "        else:\n",
    "            self.fit_map_model(loader)\n",
    "\n",
    "        # SWAG\n",
    "        if self.inference_mode in (InferenceType.SWAG_DIAGONAL, InferenceType.SWAG_FULL):\n",
    "            self.fit_swag_model(loader)\n",
    "\n",
    "    def fit_map_model(self, loader: torch.utils.data.DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        MAP inference procedure to obtain initial weights of self.network.\n",
    "        This is the exact procedure that was used to obtain the pretrained weights we provide.\n",
    "        \"\"\"\n",
    "        map_training_epochs = 140\n",
    "        initial_learning_rate = 0.01\n",
    "        reduced_learning_rate = 0.0001\n",
    "        start_decay_epoch = 50\n",
    "        decay_factor = reduced_learning_rate / initial_learning_rate\n",
    "\n",
    "        # Create optimizer, loss, and a learning rate scheduler that aids convergence\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.network.parameters(),\n",
    "            lr=initial_learning_rate,\n",
    "            momentum=0.9,\n",
    "            nesterov=False,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
    "            optimizer,\n",
    "            [\n",
    "                torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0),\n",
    "                torch.optim.lr_scheduler.LinearLR(\n",
    "                    optimizer,\n",
    "                    start_factor=1.0,\n",
    "                    end_factor=decay_factor,\n",
    "                    total_iters=(map_training_epochs - start_decay_epoch) * len(loader),\n",
    "                ),\n",
    "            ],\n",
    "            milestones=[start_decay_epoch * len(loader)],\n",
    "        )\n",
    "\n",
    "        # Put network into training mode\n",
    "        # Batch normalization layers are only updated if the network is in training mode,\n",
    "        # and are replaced by a moving average if the network is in evaluation mode.\n",
    "        self.network.train()\n",
    "        with tqdm.trange(map_training_epochs, desc=\"Fitting initial MAP weights\") as pbar:\n",
    "            progress_dict = {}\n",
    "            # Perform the specified number of MAP epochs\n",
    "            for epoch in pbar:\n",
    "                avg_loss = 0.0\n",
    "                avg_accuracy = 0.0\n",
    "                num_samples = 0\n",
    "                # Iterate over batches of randomly shuffled training data\n",
    "                for batch_images, _, _, batch_labels in loader:\n",
    "                    # Training step\n",
    "                    optimizer.zero_grad()\n",
    "                    predictions = self.network(batch_images)\n",
    "                    batch_loss = loss_fn(input=predictions, target=batch_labels)\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Save learning rate that was used for step, and calculate new one\n",
    "                    progress_dict[\"lr\"] = lr_scheduler.get_last_lr()[0]\n",
    "                    with warnings.catch_warnings():\n",
    "                        # Suppress annoying warning (that we cannot control) inside PyTorch\n",
    "                        warnings.simplefilter(\"ignore\")\n",
    "                        lr_scheduler.step()\n",
    "\n",
    "                    # Calculate cumulative average training loss and accuracy\n",
    "                    avg_loss = (batch_images.size(0) * batch_loss.item() + num_samples * avg_loss) / (\n",
    "                        num_samples + batch_images.size(0)\n",
    "                    )\n",
    "                    avg_accuracy = (\n",
    "                        torch.sum(predictions.argmax(dim=-1) == batch_labels).item()\n",
    "                        + num_samples * avg_accuracy\n",
    "                    ) / (num_samples + batch_images.size(0))\n",
    "                    num_samples += batch_images.size(0)\n",
    "\n",
    "                    progress_dict[\"avg. epoch loss\"] = avg_loss\n",
    "                    progress_dict[\"avg. epoch accuracy\"] = avg_accuracy\n",
    "                    pbar.set_postfix(progress_dict)\n",
    "\n",
    "    def predict_probabilities(self, xs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the given images xs.\n",
    "        This method returns an NxC float tensor,\n",
    "        where row i column j corresponds to the probability that y_i is class j.\n",
    "\n",
    "        This method uses different strategies depending on self.inference_mode.\n",
    "        \"\"\"\n",
    "        self.network = self.network.eval()\n",
    "\n",
    "        # Create a loader that we can deterministically iterate many times if necessary\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(xs),\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():  # save memory by not tracking gradients\n",
    "            if self.inference_mode == InferenceType.MAP:\n",
    "                return self.predict_probabilities_map(loader)\n",
    "            else:\n",
    "                return self.predict_probabilities_swag(loader)\n",
    "\n",
    "    def predict_probabilities_map(self, loader: torch.utils.data.DataLoader) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict probabilities assuming that self.network is a MAP estimate.\n",
    "        This simply performs a forward pass for every batch in `loader`,\n",
    "        concatenates all results, and applies a row-wise softmax.\n",
    "        \"\"\"\n",
    "        all_predictions = []\n",
    "        for (batch_images,) in loader:\n",
    "            all_predictions.append(self.network(batch_images))\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        return torch.softmax(all_predictions, dim=-1)\n",
    "\n",
    "    def _update_batchnorm_statistics(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset and fit batch normalization statistics using the training dataset self.training_dataset.\n",
    "        See the SWAG paper for why this is required.\n",
    "\n",
    "        Batch normalization usually uses an exponential moving average, controlled by the `momentum` parameter.\n",
    "        However, we are not training but want the statistics for the full training dataset.\n",
    "        Hence, setting `momentum` to `None` tracks a cumulative average instead.\n",
    "        The following code stores original `momentum` values, sets all to `None`,\n",
    "        and restores the previous hyperparameters after updating batchnorm statistics.\n",
    "        \"\"\"\n",
    "\n",
    "        original_momentum_values = dict()\n",
    "        for module in self.network.modules():\n",
    "            # Only need to handle batchnorm modules\n",
    "            if not isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n",
    "                continue\n",
    "\n",
    "            # Store old momentum value before removing it\n",
    "            original_momentum_values[module] = module.momentum\n",
    "            module.momentum = None\n",
    "\n",
    "            # Reset batch normalization statistics\n",
    "            module.reset_running_stats()\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.training_dataset,\n",
    "            batch_size=32,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "        self.network.train()\n",
    "        for (batch_images,) in loader:\n",
    "            self.network(batch_images)\n",
    "        self.network.eval()\n",
    "\n",
    "        # Restore old `momentum` hyperparameter values\n",
    "        for module, momentum in original_momentum_values.items():\n",
    "            module.momentum = momentum\n",
    "\n",
    "\n",
    "class SWAGScheduler(torch.optim.lr_scheduler.LRScheduler):\n",
    "    \"\"\"\n",
    "    Custom learning rate scheduler that calculates a different learning rate each gradient descent step.\n",
    "    The default implementation keeps the original learning rate constant, i.e., does nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_lr(self, current_epoch: float, previous_lr: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the learning rate for the epoch given by current_epoch.\n",
    "        current_epoch is the fractional epoch of SWA fitting, starting at 0.\n",
    "        That is, an integer value x indicates the start of epoch (x+1),\n",
    "        and non-integer values x.y correspond to steps in between epochs (x+1) and (x+2).\n",
    "        previous_lr is the previous learning rate.\n",
    "\n",
    "        This method returns a single float: the new learning rate.\n",
    "        \"\"\"\n",
    "        return previous_lr\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epochs: int,\n",
    "        steps_per_epoch: int,\n",
    "    ):\n",
    "        self.epochs = epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        super().__init__(optimizer, last_epoch=-1, verbose=False)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\n",
    "                \"To get the last learning rate computed by the scheduler, use `get_last_lr()`.\", UserWarning\n",
    "            )\n",
    "        return [\n",
    "            self.calculate_lr(self.last_epoch / self.steps_per_epoch, group[\"lr\"])\n",
    "            for group in self.optimizer.param_groups\n",
    "        ]\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    swag_inference: SWAGInference,\n",
    "    eval_dataset: torch.utils.data.Dataset,\n",
    "    extended_evaluation: bool,\n",
    "    output_location: pathlib.Path,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluate our model.\n",
    "    :param swag_inference: Trained model to evaluate\n",
    "    :param eval_dataset: Validation dataset\n",
    "    :param: extended_evaluation: If True, generates additional plots\n",
    "    :param output_location: Directory into which extended evaluation plots are saved\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Evaluating model on validation data\")\n",
    "\n",
    "    # We ignore is_snow and is_cloud here\n",
    "    images, snow_labels, cloud_labels, labels = eval_dataset.tensors\n",
    "\n",
    "    # Predict class probabilities on test data,\n",
    "    # most likely classes (according to the max predicted probability),\n",
    "    # and classes as predicted by our SWAG implementation.\n",
    "    all_pred_probabilities = swag_inference.predict_probabilities(images)\n",
    "    max_pred_probabilities, argmax_pred_labels = torch.max(all_pred_probabilities, dim=-1)\n",
    "    predicted_labels = swag_inference.predict_labels(all_pred_probabilities)\n",
    "\n",
    "    # Create a mask that ignores ambiguous samples (those with class -1)\n",
    "    non_ambiguous_mask = labels != -1\n",
    "\n",
    "    # Calculate three kinds of accuracy:\n",
    "    # 1. Overall accuracy, counting \"don't know\" (-1) as its own class\n",
    "    # 2. Accuracy on all samples that have a known label. Predicting -1 on those counts as wrong here.\n",
    "    # 3. Accuracy on all samples that have a known label w.r.t. the class with the highest predicted probability.\n",
    "    overall_accuracy = torch.mean((predicted_labels == labels).float()).item()\n",
    "    non_ambiguous_accuracy = torch.mean((predicted_labels[non_ambiguous_mask] == labels[non_ambiguous_mask]).float()).item()\n",
    "    non_ambiguous_argmax_accuracy = torch.mean(\n",
    "        (argmax_pred_labels[non_ambiguous_mask] == labels[non_ambiguous_mask]).float()\n",
    "    ).item()\n",
    "    print(f\"Accuracy (raw): {overall_accuracy:.4f}\")\n",
    "    print(f\"Accuracy (non-ambiguous only, our predictions): {non_ambiguous_accuracy:.4f}\")\n",
    "    print(f\"Accuracy (non-ambiguous only, predicting most-likely class): {non_ambiguous_argmax_accuracy:.4f}\")\n",
    "\n",
    "    # Determine which threshold would yield the smallest cost on the validation data\n",
    "    threshold_values = [0.0] + list(torch.unique(max_pred_probabilities, sorted=True))\n",
    "    costs = []\n",
    "    for threshold in threshold_values:\n",
    "        thresholded_predictions = torch.where(max_pred_probabilities <= threshold, -1 * torch.ones_like(predicted_labels), predicted_labels)\n",
    "        costs.append(cost_function(thresholded_predictions, labels).item())\n",
    "    best_threshold_index = np.argmin(costs)\n",
    "    print(f\"Best cost {costs[best_threshold_index]} at threshold {threshold_values[best_threshold_index]}\")\n",
    "    print(\"Note that this threshold does not necessarily generalize to the test set\")\n",
    "\n",
    "    # Calculate ECE and plot the calibration curve\n",
    "    calibration_data = calc_calibration_curve(all_pred_probabilities.numpy(), labels.numpy(), num_bins=20)\n",
    "    print(\"Validation ECE:\", calibration_data[\"ece\"])\n",
    "\n",
    "    if extended_evaluation:\n",
    "        print(\"Plotting reliability diagram\")\n",
    "        fig = draw_reliability_diagram(calibration_data)\n",
    "        fig.savefig(output_location / \"reliability_diagram.pdf\")\n",
    "\n",
    "        sorted_confidence_indices = torch.argsort(max_pred_probabilities)\n",
    "\n",
    "        # Plot samples our model is most confident about\n",
    "        print(\"Plotting most confident validation set predictions\")\n",
    "        most_confident_indices = sorted_confidence_indices[-10:]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_index = most_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(images[sample_index].permute(1, 2, 0).numpy())\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f\"pred. {predicted_labels[sample_index]}, true {labels[sample_index]}\")\n",
    "                bar_colors = [\"C0\"] * 6\n",
    "                if labels[sample_index] >= 0:\n",
    "                    bar_colors[labels[sample_index]] = \"C1\"\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(6), all_pred_probabilities[sample_index].numpy(), tick_label=np.arange(6), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle(\"Most confident predictions\", size=20)\n",
    "        fig.savefig(output_location / \"examples_most_confident.pdf\")\n",
    "\n",
    "        # Plot samples our model is least confident about\n",
    "        print(\"Plotting least confident validation set predictions\")\n",
    "        least_confident_indices = sorted_confidence_indices[:10]\n",
    "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
    "        for row in range(0, 4, 2):\n",
    "            for col in range(5):\n",
    "                sample_index = least_confident_indices[5 * row // 2 + col]\n",
    "                ax[row, col].imshow(images[sample_index].permute(1, 2, 0).numpy())\n",
    "                ax[row, col].set_axis_off()\n",
    "                ax[row + 1, col].set_title(f\"pred. {predicted_labels[sample_index]}, true {labels[sample_index]}\")\n",
    "                bar_colors = [\"C0\"] * 6\n",
    "                if labels[sample_index] >= 0:\n",
    "                    bar_colors[labels[sample_index]] = \"C1\"\n",
    "                ax[row + 1, col].bar(\n",
    "                    np.arange(6), all_pred_probabilities[sample_index].numpy(), tick_label=np.arange(6), color=bar_colors\n",
    "                )\n",
    "        fig.suptitle(\"Least confident predictions\", size=20)\n",
    "        fig.savefig(output_location / \"examples_least_confident.pdf\")\n",
    "\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Small convolutional neural network used in this task.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_classes: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer0 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, 32, kernel_size=5),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.pool1 = torch.nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.pool2 = torch.nn.MaxPool2d((2, 2), stride=(2, 2))\n",
    "\n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3),\n",
    "        )\n",
    "\n",
    "        self.global_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.linear = torch.nn.Linear(64, out_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.layer5(x)\n",
    "\n",
    "        # Average features over both spatial dimensions, and remove the now superfluous dimensions\n",
    "        x = self.global_pool(x).squeeze(-1).squeeze(-1)\n",
    "\n",
    "        log_softmax = self.linear(x)\n",
    "\n",
    "        return log_softmax\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33919b8-8aa4-4b80-855f-5aa7f7202c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
